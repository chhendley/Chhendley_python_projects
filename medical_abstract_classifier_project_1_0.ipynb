{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chhendley/Chhendley-portfolio-python/blob/main/medical_abstract_classifier_project_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKLWkhoqNEKW",
        "outputId": "b7557431-e57d-4f38-cc98-c675db66d0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!pip install keras-tuner -q\n",
        "#installing the required libraries\n",
        "import keras_tuner\n",
        "from tensorflow import keras\n",
        "from keras_tuner import RandomSearch, GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcE3LM5G_97J",
        "outputId": "f1ac6552-4888-452b-dd2b-e30ecd7c025b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scispacy\n",
            "  Downloading scispacy-0.5.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy<3.5.0,>=3.4.0 (from scispacy)\n",
            "  Downloading spacy-3.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scispacy) (2.27.1)\n",
            "Collecting conllu (from scispacy)\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.22.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.2.0)\n",
            "Collecting nmslib>=1.7.3.6 (from scispacy)\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from scispacy) (1.2.2)\n",
            "Collecting pysbd (from scispacy)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11<2.6.2 (from nmslib>=1.7.3.6->scispacy)\n",
            "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.9.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.3->scispacy) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (8.1.9)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.5.0,>=3.4.0->scispacy)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (4.65.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.0->scispacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->scispacy) (4.5.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->scispacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->scispacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->scispacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->scispacy) (2.1.2)\n",
            "Building wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.1-cp310-cp310-linux_x86_64.whl size=13494892 sha256=296e6f245a1aa5c72e099852e67aac3ccc52506f82fca4ce81cb13a29e05f770\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/1a/5d/4cc754a5b1a88405cad184b76f823897a63a8d19afcd4b9314\n",
            "Successfully built nmslib\n",
            "Installing collected packages: wasabi, pysbd, pybind11, conllu, nmslib, spacy, scispacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed conllu-4.5.2 nmslib-2.1.1 pybind11-2.6.1 pysbd-0.3.4 scispacy-0.5.2 spacy-3.4.4 wasabi-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install scispacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHuO9FVXACtf",
        "outputId": "55e848d8-4233-4ab3-e1a5-5942a9d2ed07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz (120.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /usr/local/lib/python3.10/dist-packages (from en-core-sci-md==0.5.1) (3.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-sci-md==0.5.1) (2.1.2)\n",
            "Building wheels for collected packages: en-core-sci-md\n",
            "  Building wheel for en-core-sci-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-sci-md: filename=en_core_sci_md-0.5.1-py3-none-any.whl size=120253146 sha256=0efbc6fc0c7856230feb68de379661e04518016addbe99d768d9a6e3f378f267\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/08/9d/5a05b2b428c07c244a314064fc57a966bcc871b4164590df4d\n",
            "Successfully built en-core-sci-md\n",
            "Installing collected packages: en-core-sci-md\n",
            "Successfully installed en-core-sci-md-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_md-0.5.1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuGtUrKhFDKt",
        "outputId": "aa4855b9-e9c8-4813-cffe-1f5cce25d991"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from keras.layers import Input, Dense, LSTM, Embedding\n",
        "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Sequential\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.layers import BatchNormalization, LayerNormalization\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import keras\n",
        "from keras.layers.core.dense import utils\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.compose import ColumnTransformer \n",
        "from sklearn.preprocessing import OneHotEncoder \n",
        "from keras.wrappers.scikit_learn import KerasClassifier \n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "from sklearn.preprocessing import StandardScaler \n",
        "import spacy\n",
        "import scispacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.saving.legacy.saving_utils import model_call_inputs\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "stopwords = stopwords.words('english')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmKaQZSgyvwE"
      },
      "outputs": [],
      "source": [
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp = spacy.load('en_core_sci_md')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnlcS2Az8erP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "copied_path = ('drive/MyDrive/Colab Notebooks/processing_train.csv') #remove ‘content/’ from path then use \n",
        "df = pd.read_csv(copied_path)\n",
        "\n",
        "#original_df = pd.read_csv(io.BytesIO(uploaded['processing_train.csv']))\n",
        "#original_df = pd.read_csv('train.dat',sep='::')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "acmBykL5vruw"
      },
      "outputs": [],
      "source": [
        "#types of classes: 2)digestive system diseases, 4)cardiovascular diseases, 1)neoplasms, 3)nervous system diseases, and 5)general pathological conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aXZf6f7AFALl"
      },
      "outputs": [],
      "source": [
        "raw_df = df.drop(df.columns[[0, 1]],axis = 1)\n",
        "\n",
        "display(raw_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x2vfZukhoGys"
      },
      "outputs": [],
      "source": [
        "def train_df(df_a, x):\n",
        "  df_b = df_a.copy()\n",
        "  df = df_b[df_b['Labels']==x]\n",
        "  df.reset_index(inplace=True)\n",
        "  df.drop(df.index[1494:], inplace=True)\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UfMO-7F5yEfA"
      },
      "outputs": [],
      "source": [
        "#run train_df on all 5 categories and concat into single df for training\n",
        "\n",
        "#1494 max length of dfs\n",
        "raw_df_1 = train_df(raw_df, 1)\n",
        "raw_df_2 = train_df(raw_df, 2)\n",
        "raw_df_3 = train_df(raw_df, 3)\n",
        "raw_df_4 = train_df(raw_df, 4)\n",
        "raw_df_5 = train_df(raw_df, 5)\n",
        "original_df = pd.concat([raw_df_1, raw_df_2, raw_df_3, raw_df_4, raw_df_5])\n",
        "display(original_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ArOHe-AeY6pb"
      },
      "outputs": [],
      "source": [
        "original_df['text_spacy']= original_df['text_spacy'].apply(nlp)\n",
        "#original_df['text_token'] = original_df['text_token'].apply(lambda x: [item for item in x if item not in stpwrd])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NEBMI1fvA8eQ"
      },
      "outputs": [],
      "source": [
        "display(original_df['text_spacy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ppTStxwiRkk4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_non_numberics(s):\n",
        "    return re.sub('[^a-zA-Z\\s]', '', s) \n",
        " \n",
        "original_df['Text'] = original_df['Text'].apply(remove_non_numberics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BgKsZIkwqIy7"
      },
      "outputs": [],
      "source": [
        "#use Keras tokenizer to tokenize dataset\n",
        "data = original_df['Text'].map(text_to_word_sequence).values\n",
        "total_vocabulary = set(word.lower() for tweet in data for word in tweet)  # set created from nested comprehension \n",
        "print('There are {} unique words in the dataset.'.format(len(total_vocabulary)))\n",
        "print('There are {} unique abstracts in the dataset.'.format(len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mLmyuDw7rrOm"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# Ngrams allows to group words in common pairs or trigrams..etc\n",
        "from nltk import ngrams\n",
        "# We can use counter to count the objects from collections\n",
        "from nltk import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4qGLWqceFvUe"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for i in range(len(original_df.index)):\n",
        "  g = len(original_df.loc[i, 'text_spacy']) \n",
        "  data.append(g)\n",
        "\n",
        "fig = plt.figure(figsize =(10, 7))\n",
        " \n",
        "# Creating plot\n",
        "plt.boxplot(data)\n",
        " \n",
        "# show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ETVAQDrHwQDx"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers.core.dense import utils\n",
        "from keras.utils import pad_sequences\n",
        "#set the emotion/sentiment as our target\n",
        "target = original_df['Labels']\n",
        "\n",
        "# use one hot encoding since our target is categorical\n",
        "y = pd.get_dummies(target).values\n",
        "\n",
        "# use keras to create a Tokenizer object\n",
        "tokenizer = text.Tokenizer(num_words=38625)  # limit to the num_words most important ones\n",
        "tokenizer.fit_on_texts(list(original_df['Text']))\n",
        "tokenized_texts = tokenizer.texts_to_sequences(original_df['Text'])\n",
        "X = keras.utils.pad_sequences(tokenized_texts, maxlen=2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vxfkeOmjTC2_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "stopwords = stopwords.words('english')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_list = ['subject', 'feature', 'child','patient', 'patients', 'case', 'cases','group', 'disease', 'cell', 'study','studies', 'p', 'le', 'case', 'treatment', 'treated', 'year','years',  'two', 'may', 'result','results', 'one',\n",
        "       'rate', 'effect','effects','clinical', 'control', 'mean', 'therapy', 'level', 'levels',\n",
        "       'normal', 'month','months', 'associated', 'pressure','also', 'analysis'\n",
        "       'significantly', 'three', 'risk', 'significant',\n",
        "       'factor', 'increased', 'age', 'compared', 'time', 'lesion','left', 'total', 'analysis'\n",
        "       'also', 'diagnosis', 'found', 'change', 'syndrome','pain', 'symptoms',\n",
        "       'surgery', 'less', 'greater', 'without', 'used', 'using', 'showed', 'lesions', 'groups', 'performed', 'children', 'four', 'use', 'high', 'human', 'however', 'followup', 'per']\n",
        "stpwrd = nltk.corpus.stopwords.words('english')\n",
        "# entend()function is used to add custom stopwords \n",
        "for i in stop_list:\n",
        "  stpwrd.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6DXtBnBf1mt9"
      },
      "outputs": [],
      "source": [
        "df_1 = original_df[original_df['Labels']=='1']\n",
        "df_2 = original_df[original_df['Labels']=='2']\n",
        "df_3 = original_df[original_df['Labels']=='3']\n",
        "df_4 = original_df[original_df['Labels']=='4']\n",
        "df_5 = original_df[original_df['Labels']=='5']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lNUwh_7vVANd"
      },
      "outputs": [],
      "source": [
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# set the emotion/sentiment as our target\n",
        "target = original_df['Labels']\n",
        "\n",
        "# use one hot encoding since our target is categorical\n",
        "y = pd.get_dummies(target).values\n",
        "\n",
        "original_df['text'] = original_df['Text'].astype(str).str.lower()\n",
        "regexp = RegexpTokenizer('\\w+')\n",
        "original_df['text_token']= original_df['text'].apply(regexp.tokenize)\n",
        "original_df['text_token'] = original_df['text_token'].apply(lambda x: [item for item in x if item not in stpwrd])\n",
        "original_df['text_string'] = original_df['text_token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
        "all_words = ' '.join([word for word in original_df['text_string']])\n",
        "tokenized_words = nltk.tokenize.word_tokenize(all_words)\n",
        "fdist = FreqDist(tokenized_words)\n",
        "wordnet_lem = WordNetLemmatizer()\n",
        "original_df['text_string_lem'] = original_df['text_string'].apply(wordnet_lem.lemmatize)\n",
        "all_words_lem = ' '.join([word for word in original_df['text_string_lem']])\n",
        "words = nltk.word_tokenize(all_words_lem)\n",
        "fd = FreqDist(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "expzRHz1ZxQE"
      },
      "outputs": [],
      "source": [
        "pandas_list = original_df['text_string_lem'].values.tolist()\n",
        "tokenized_texts = tokenizer.texts_to_sequences(pandas_list)\n",
        "#X_pandas = tf.keras.utils.pad_sequences(tokenized_texts, maxlen=2000) original \n",
        "X_pandas = tf.keras.utils.pad_sequences(tokenized_texts, maxlen=200) #shortened text space "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QRM-P17xXQzu"
      },
      "outputs": [],
      "source": [
        "# Obtain top 10 words\n",
        "top_10 = fd.most_common(30)\n",
        "\n",
        "# Create pandas series to make plotting easier\n",
        "fdist = pd.Series(dict(top_10))\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"ticks\")\n",
        "\n",
        "sns.barplot(y=fdist.index, x=fdist.values, color='red');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xlWgrwmzzEDN"
      },
      "outputs": [],
      "source": [
        "# set the emotion/sentiment as our target\n",
        "df = df_1.copy()\n",
        "\n",
        "target = df['Labels']\n",
        "\n",
        "# use one hot encoding since our target is categorical\n",
        "y = pd.get_dummies(target).values\n",
        "\n",
        "df['text'] = df['Text'].astype(str).str.lower()\n",
        "regexp = RegexpTokenizer('\\w+')\n",
        "df['text_token']= df['text'].apply(regexp.tokenize)\n",
        "df['text_token'] = df['text_token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "df['text_string'] = df['text_token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
        "all_words = ' '.join([word for word in df['text_string']])\n",
        "tokenized_words = nltk.tokenize.word_tokenize(all_words)\n",
        "fdist = FreqDist(tokenized_words)\n",
        "wordnet_lem = WordNetLemmatizer()\n",
        "df['text_string_lem'] = df['text_string'].apply(wordnet_lem.lemmatize)\n",
        "all_words_lem = ' '.join([word for word in original_df['text_string_lem']])\n",
        "words = nltk.word_tokenize(all_words_lem)\n",
        "fd = FreqDist(words)\n",
        "\n",
        "print(\"number of samples in category\", len(df.index))\n",
        "\n",
        "# Obtain top 10 words\n",
        "top_words = fd #.most_common(10)\n",
        "\n",
        "# Create pandas series to make plotting easier\n",
        "fdist = pd.Series(dict(top_words))\n",
        "\n",
        "display(fdist)\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8IrbzW1KacXm"
      },
      "outputs": [],
      "source": [
        "#use Keras tokenizer to tokenize dataset\n",
        "data = original_df['text_string_lem'].map(text_to_word_sequence).values\n",
        "total_vocabulary = set(word.lower() for tweet in data for word in tweet)  # set created from nested comprehension \n",
        "print('There are {} unique words in the dataset.'.format(len(total_vocabulary)))\n",
        "print('There are {} unique abstracts in the dataset.'.format(len(data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-PGVJocjXS6"
      },
      "source": [
        "with stopwords added\n",
        "\n",
        "\n",
        "\n",
        "*   5 epoch 2500 word batchsize(20) batchnormalization 55.58% accuracy, val accuracy 53.03%; 3 LMST layers 50 each; used NLT+custom words to remove stop words; drop out .50\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx2vrWeUhxnP"
      },
      "source": [
        "Without stopwords extended\n",
        "\n",
        "*   1 epoch 3950 word limit batchsize(20) no batch normalization 53% accuracy\n",
        "*   5 epoch 3950 word limit batchsize(20) no batch normalization 66% accuracy\n",
        "*   5 epoch 3950 word limit batchsize(20) batch normalization 52% accuracy; \n",
        "*   5 epoch 3950 word batchsize(20) batchnormalization 63% accuracy; increased LMST from 25 to 50\n",
        "*   10 epoch 3950 word batchsize(20) batchnormalization 69.54% accuracy; increased LMST 50\n",
        "*   10 epoch 2500 word batchsize(20) batchnormalization 71.11% accuracy; increased LMST 50; used NLT to remove stop words\n",
        "*   10 epoch 2500 word batchsize(20) batchnormalization 65.70% accuracy; increased LMST 50; used NLT+custom words to remove stop words\n",
        "*   15 epoch 2500 word batchsize(20) batchnormalization 65.79.35% accuracy; increased LMST 50; used NLT+custom words to remove stop words\n",
        "*   5 epoch 2500 word batchsize(20) batchnormalization 62.79.35% accuracy; increased LMST 50; used NLT+custom words to remove stop words; Elu activation fx\n",
        "*   5 epoch 2500 word batchsize(20) batchnormalization 65.79.35% accuracy; increased LMST 50; used NLT+custom words to remove stop words; 100 LMSTM\n",
        "*   5 epoch 2500 word batchsize(20) batchnormalization 68.46% accuracy, val accuracy 58.26%; 3 LMST layers 100 each; used NLT+custom words to remove stop words; drop out .25\n",
        "*   5 epoch 2500 word batchsize(20) batchnormalization 73.74% accuracy, val accuracy 53.51%; 3 LMST layers 100 each; used NLT+custom words to remove stop words; drop out .15\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZFI9PT0iIc3"
      },
      "source": [
        "hyperparameter grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vj17ruUam5qz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "param_search_data = original_df#.iloc[0:3611]\n",
        "target_p = param_search_data['Labels'] #.loc[0:3611, 'Labels']\n",
        "y_param = pd.get_dummies(target_p).values\n",
        "#y_minmax = scaler.fit_tranform(param_search_data['Labels'])\n",
        "param_list = param_search_data['text_string_lem'].values.tolist()\n",
        "tokenized_texts = tokenizer.texts_to_sequences(param_list)\n",
        "X_param = tf.keras.utils.pad_sequences(tokenized_texts, maxlen=400)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_param, y_param, test_size=0.2, random_state=7, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1joeir0xZSRb"
      },
      "outputs": [],
      "source": [
        "label_data = param_search_data['Labels'].value_counts(ascending=True)\n",
        "print(label_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9TYfLLkntk2T"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "!pip install keras-tuner -q\n",
        "#installing the required libraries\n",
        "import keras_tuner\n",
        "from tensorflow import keras\n",
        "from keras_tuner import RandomSearch, GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iuc-_4J0rf5M"
      },
      "outputs": [],
      "source": [
        "\n",
        "#5 epoch 2500 word batchsize(20) batchnormalization 68.46% accuracy, val accuracy 58.26%; 3 LMST layers 100 each; used NLT+custom words to remove stop words; drop out .25\n",
        "input = (len(total_vocabulary) +1)\n",
        "\n",
        "def build_model(hp):   \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim = input, output_dim=hp.Int('embed_neurons',min_value=100,max_value=1300,step=100)))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dropout(rate = hp.Float('D1_rate',min_value=.15,max_value=0.35,step=0.10)))\n",
        "  model.add(LSTM(units = hp.Int('L1_neurons',min_value=50,max_value=250,step=100), return_sequences=True))\n",
        "  model.add(LSTM(units = hp.Int('L2_neurons',min_value=50,max_value=250,step=100), return_sequences=True))\n",
        "  model.add(LSTM(units = hp.Int('L3_neurons',min_value=50,max_value=250,step=100), return_sequences=True))\n",
        "  model.add(GlobalMaxPool1D())\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dropout(rate = hp.Float('D2_rate',min_value=.15,max_value=0.35,step=0.10)))\n",
        "  model.add(Dense(units = hp.Int('Dense1_neurons',min_value=100,max_value=1000,step=100), activation= 'selu'))\n",
        "  model.add(Dense(5, activation='softmax'))  # use 5 because we have 5 categories\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model_a = build_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LHA5M-4L1u7t"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "tuner_h = kt.RandomSearch(model_a,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=10,\n",
        "                     directory='/content/drive/MyDrive/Colab Notebooks/model tuning',\n",
        "                     project_name='RNN modelb')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "tuner_h.search(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[stop_early])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fzoHYDT2tYFw"
      },
      "outputs": [],
      "source": [
        "input = (len(total_vocabulary) +1)\n",
        "\n",
        "def build_model(hp):   \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim = input, output_dim=hp.Int('embed_neurons',min_value=900,max_value=1100,step=100)))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dropout(rate = hp.Float('D1_rate',min_value=.10,max_value=0.20,step=0.05)))\n",
        "  model.add(LSTM(units = hp.Int('L1_neurons',min_value=50,max_value=200,step=50), return_sequences=True))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dropout(rate = hp.Float('D2_rate',min_value=.20,max_value=0.30,step=0.05)))\n",
        "  model.add(LSTM(units = hp.Int('L2_neurons',min_value=200,max_value=500,step=100), return_sequences=True))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dropout(rate = hp.Float('D3_rate',min_value=.10,max_value=0.20,step=0.05)))\n",
        "  model.add(LSTM(units = hp.Int('L3_neurons',min_value=50,max_value=200,step=50), return_sequences=True))\n",
        "  model.add(GlobalMaxPool1D())\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dropout(rate = hp.Float('D4_rate',min_value=.10,max_value=0.20,step=0.05)))\n",
        "  model.add(Dense(units = hp.Int('Dense1_neurons',min_value=200,max_value=400,step=100), activation= 'relu'))\n",
        "  model.add(Dense(5, activation='softmax'))  # use 5 because we have 5 categories\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model_j = build_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDLgqTbqownJ"
      },
      "source": [
        "compare categorical cross entropy loss vs spare\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5 epoch 2500 word batchsize(20) batchnormalization 68.46% accuracy, val accuracy 58.26%; 3 LMST layers 100 each; used NLT+custom words to remove stop words; drop out .25\n",
        "\n",
        "\n",
        "Best val_accuracy So Far: 0.5674740672111511\n",
        "Total elapsed time: 00h 51m 51s\n",
        "embed nerons 1000\n",
        "L1_nerons 1000\n",
        "D1 rate 0.30000000000000004\n",
        "D2 rate 0.2\n",
        "Dense1 nerons 600\n",
        "Activation fx selu\n",
        "optimizer 'Adam'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oD9hyXxWqcoV"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "tuner_i = kt.GridSearch(model_j,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=10,\n",
        "                     directory='/content/drive/MyDrive/Colab Notebooks/model tuning',\n",
        "                     project_name='RNN modele')\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "tuner_i.search(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RMKM3K_8r3yz"
      },
      "outputs": [],
      "source": [
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner_i.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"embed nerons\", best_hps.get('embed_neurons'))\n",
        "print(\"L1_nerons\", best_hps.get('L1_neurons'))\n",
        "print(\"L2_nerons\", best_hps.get('L2_neurons'))\n",
        "print(\"L3_nerons\", best_hps.get('L3_neurons'))\n",
        "print(\"D1 rate\", best_hps.get('D1_rate'))\n",
        "print(\"D2 rate\", best_hps.get('D2_rate'))\n",
        "print(\"D3 rate\", best_hps.get('D3_rate'))\n",
        "print(\"Dense1 nerons\", best_hps.get('Dense1_neurons'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pivl8b8HGthV"
      },
      "outputs": [],
      "source": [
        "# Save the weights\n",
        "model.save_weights('/content/drive/MyDrive/Colab Notebooks/saved models/modelc.h5') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VJqi8_JAG9Rr"
      },
      "outputs": [],
      "source": [
        "# Load the previously saved weights\n",
        "#model.load_weights('/content/drive/MyDrive/Colab Notebooks/saved models/modelb.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7pev8YSxq8Cw"
      },
      "outputs": [],
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0QHoa6vkrAkf"
      },
      "outputs": [],
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LN_1qIPcrDGk"
      },
      "outputs": [],
      "source": [
        "#eval_result = hypermodel.evaluate(X_test, y_test)\n",
        "eval_result = model.evaluate(X_val, y_val)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7S8ZOfcByocw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_pandas, y, test_size=0.2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_param, y_param, test_size=0.2, random_state=7, shuffle=True)\n",
        "# create my NN model\n",
        "model = Sequential()\n",
        "\n",
        "embedding_size = 500 #2940\n",
        "model.add(Embedding((len(total_vocabulary) +1), embedding_size))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(LayerNormalization())\n",
        "model.add(Dense(10, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.25))\n",
        "#model.add(Dense(150, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.50))\n",
        "model.add(Dense(5, activation='softmax'))  # use 5 because we have 5 categories\n",
        "\n",
        "#5 epoch 2500 word batchsize(20) batchnormalization 68.46% accuracy, val accuracy 58.26%; 3 LMST layers 100 each; used NLT+custom words to remove stop words; drop out .25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lS0xYLR52Hm4"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary() # check the shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gM6Guva32NpT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Include the epoch in the file name (uses `str.format`)\n",
        "#checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
        "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "Batch_size =20\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_weights_only=True, save_freq=5*Batch_size)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=Batch_size, validation_split=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_L12U2hX2RvO"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = model.predict(X_test) # get our predictions\n",
        "Y_pred = np.rint(y_pred)\n",
        "acc = accuracy_score(y_test, Y_pred) \n",
        "print('Overall accuracy of RNN: {:.3f}'.format(acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zkZe1U25_AGv"
      },
      "outputs": [],
      "source": [
        "from keras.saving.legacy.saving_utils import model_call_inputs\n",
        "\n",
        "# Create a new model instance\n",
        "modela = model\n",
        "\n",
        "# Load the previously saved weights\n",
        "modela.load_weights('/content/drive/MyDrive/Colab Notebooks/saved models/modelb.h5')\n",
        "\n",
        "# Re-evaluate the model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))\n",
        "\n",
        "y_pred = modela.predict(X_test) # get our predictions\n",
        "Y_pred = np.rint(y_pred)\n",
        "acc = accuracy_score(y_test, Y_pred) \n",
        "print('Overall accuracy of RNN: {:.3f}'.format(acc))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1uaJsllKdm08oCDKmGYuQegk4qxEziIZh",
      "authorship_tag": "ABX9TyNxIDehkFltOu/JO+IzWmSx",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}